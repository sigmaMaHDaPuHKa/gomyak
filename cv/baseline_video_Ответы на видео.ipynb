{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "В задаче Video Question Answering модели необходимо проанализировать короткие видеофрагменты и представленные к ним вопросы по содержанию и запечатленным событиям и/или действиям, чтобы сгенерировать наиболее подходящие ответы\n",
        "на русском языке.\n",
        "\n",
        "Участникам был предоставлен обучающий набор данных, который включал в себя 2700+ коротких видео и соответствующих им вопросов и ответов на русском языке, а также базовое решение от разработчиков с докером и примером загружаемого решения.\n",
        "\n",
        "Так как участники не имели доступа к проверяющему датасету, в качестве решения на платформу нужно было загрузить обученную модель. Решения на проверяющей платформе запускались в изолированном окружении при помощи Docker. Время\n",
        "и ресурсы во время тестирования ограничены.\n"
      ],
      "metadata": {
        "id": "DOUGxGMmsq3p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "НТО ЗАДАЧА ФИНАЛА: https://ntcontest.ru/docs/ai-assignements1.pdf"
      ],
      "metadata": {
        "id": "LJobV7YTsuwr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSIib9XGY6Fs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8bce043-c816-4010-9523-5a676ef2555b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-hncikbdd\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-hncikbdd\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a9b1bf5920416aaeaec965c25dd9e8f98c864f16\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 KB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from clip==1.0) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from clip==1.0) (4.64.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from clip==1.0) (1.13.1+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from clip==1.0) (0.14.1+cu116)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.8/dist-packages (from ftfy->clip==1.0) (0.2.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->clip==1.0) (4.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision->clip==1.0) (2.25.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision->clip==1.0) (1.22.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->clip==1.0) (8.4.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->clip==1.0) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->clip==1.0) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->clip==1.0) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->clip==1.0) (1.26.14)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369408 sha256=4aa3302c3a2d833768f01cde4f12c9aef4c5a75439e4b3089bbab0211fb6818b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-0ua66vfq/wheels/ab/4f/3a/5e51521b55997aa6f0690e095c08824219753128ce8d9969a3\n",
            "Successfully built clip\n",
            "Installing collected packages: ftfy, clip\n",
            "Successfully installed clip-1.0 ftfy-6.1.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.12.1 tokenizers-0.13.2 transformers-4.26.1\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8m4D94FWcTq6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6839e14-cd3d-422f-9046-533f25d94601"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open aa.zip, aa.zip.zip or aa.zip.ZIP.\n"
          ]
        }
      ],
      "source": [
        "!unzip aa.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0HEiMnKroae"
      },
      "outputs": [],
      "source": [
        "!pip install wandb\n",
        "!wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAGDCFZ6aOCA"
      },
      "outputs": [],
      "source": [
        "import clip\n",
        "import os\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as nnf\n",
        "import sys\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from tqdm import tqdm, trange\n",
        "import PIL.Image\n",
        "from IPython.display import Image\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85FkKUgCaQLz"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, sizes, bias=True, act=nn.Tanh):\n",
        "        super(MLP, self).__init__()\n",
        "        layers = []\n",
        "        for i in range(len(sizes) -1):\n",
        "            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=bias))\n",
        "            if i < len(sizes) - 2:\n",
        "                layers.append(act())\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "class ClipCaptionModel(nn.Module):\n",
        "\n",
        "    def __init__(self, prefix_length, prefix_size: int = 768):\n",
        "        super(ClipCaptionModel, self).__init__()\n",
        "        self.prefix_length = prefix_length\n",
        "\n",
        "        self.gpt = GPT2LMHeadModel.from_pretrained('sberbank-ai/rugpt3large_based_on_gpt2')\n",
        "\n",
        "        self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n",
        "\n",
        "        if prefix_length > 10:  # not enough memory\n",
        "            self.clip_project = nn.Linear(10,47)#prefix_size, self.gpt_embedding_size * prefix_length)\n",
        "        else:\n",
        "            self.clip_project = MLP((prefix_size, (self.gpt_embedding_size * prefix_length) // 2, self.gpt_embedding_size * prefix_length))\n",
        "\n",
        "    #@functools.lru_cache #FIXME\n",
        "    def get_dummy_token(self, batch_size, device):\n",
        "        return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)\n",
        "\n",
        "    def forward(self, tokens, prefix, mask, labels):\n",
        "        embedding_text = self.gpt.transformer.wte(tokens)\n",
        "        prefix_projections = self.clip_project(prefix).view(-1, self.prefix_length, self.gpt_embedding_size)\n",
        "        #print(embedding_text.size()) #torch.Size([5, 67, 768])\n",
        "        #print(prefix_projections.size()) #torch.Size([5, 1, 768])\n",
        "        embedding_cat = torch.cat((prefix_projections, embedding_text), dim=1)\n",
        "        if labels is not None:\n",
        "            dummy_token = self.get_dummy_token(tokens.shape[0], tokens.device)\n",
        "            labels = torch.cat((dummy_token, tokens), dim=1)\n",
        "        out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=mask)\n",
        "        return out\n",
        "\n",
        "class ClipCaptionPrefix(ClipCaptionModel):\n",
        "\n",
        "    def parameters(self, recurse = True):\n",
        "        return self.clip_project.parameters()\n",
        "\n",
        "    def train(self, mode = True):\n",
        "        super(ClipCaptionPrefix, self).train(mode)\n",
        "        self.gpt.eval()\n",
        "        return self"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S21C4pkyarFN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "f628a3f8-90b8-47e1-bfa1-52189849f28a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Unnamed: 0   video_name  \\\n",
              "0               0  okC18bPTils   \n",
              "1               1  AAQp3iEJxJc   \n",
              "2               2  whcxrkF2hws   \n",
              "3               3  LrputIUn4oY   \n",
              "4               4  Mzt-E6pxuUI   \n",
              "...           ...          ...   \n",
              "25223       25223  nIpT_lGpjck   \n",
              "25224       25224  pQof_-tUNtY   \n",
              "25225       25225  pQof_-tUNtY   \n",
              "25226       25226  bxxSc2m08Sk   \n",
              "25227       25227  r10jGF9lQYM   \n",
              "\n",
              "                                             question        answer  \n",
              "0                            все люди в видео мужчины           нет  \n",
              "1                            все люди в видео женщины           нет  \n",
              "2                  спортсмен №3, играющий в помещении           нет  \n",
              "3                                  спортсмен в брюках           нет  \n",
              "4                                 ребенок в помещении            да  \n",
              "...                                               ...           ...  \n",
              "25223                           чаша круглая на видео            да  \n",
              "25224                                  коробка черная            да  \n",
              "25225                                 коробка красная           нет  \n",
              "25226  мальчик в розовом стоит или сидит на корточках  на корточках  \n",
              "25227                  мальчик ловит рыбу в помещении           нет  \n",
              "\n",
              "[25228 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8103668b-fb85-45c2-89e9-4f4b535624cf\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>video_name</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>okC18bPTils</td>\n",
              "      <td>все люди в видео мужчины</td>\n",
              "      <td>нет</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>AAQp3iEJxJc</td>\n",
              "      <td>все люди в видео женщины</td>\n",
              "      <td>нет</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>whcxrkF2hws</td>\n",
              "      <td>спортсмен №3, играющий в помещении</td>\n",
              "      <td>нет</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>LrputIUn4oY</td>\n",
              "      <td>спортсмен в брюках</td>\n",
              "      <td>нет</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Mzt-E6pxuUI</td>\n",
              "      <td>ребенок в помещении</td>\n",
              "      <td>да</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25223</th>\n",
              "      <td>25223</td>\n",
              "      <td>nIpT_lGpjck</td>\n",
              "      <td>чаша круглая на видео</td>\n",
              "      <td>да</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25224</th>\n",
              "      <td>25224</td>\n",
              "      <td>pQof_-tUNtY</td>\n",
              "      <td>коробка черная</td>\n",
              "      <td>да</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25225</th>\n",
              "      <td>25225</td>\n",
              "      <td>pQof_-tUNtY</td>\n",
              "      <td>коробка красная</td>\n",
              "      <td>нет</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25226</th>\n",
              "      <td>25226</td>\n",
              "      <td>bxxSc2m08Sk</td>\n",
              "      <td>мальчик в розовом стоит или сидит на корточках</td>\n",
              "      <td>на корточках</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25227</th>\n",
              "      <td>25227</td>\n",
              "      <td>r10jGF9lQYM</td>\n",
              "      <td>мальчик ловит рыбу в помещении</td>\n",
              "      <td>нет</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>25228 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8103668b-fb85-45c2-89e9-4f4b535624cf')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8103668b-fb85-45c2-89e9-4f4b535624cf button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8103668b-fb85-45c2-89e9-4f4b535624cf');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "df_train = pd.read_csv('updatedtrain.csv')\n",
        "df_train"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(df_train['video_name'].unique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jy-86EIgpiT6",
        "outputId": "d68608aa-472f-40f2-beb1-99b1a58d61c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2757"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRjeRPvia6uv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "data = []\n",
        "for video_name, question, answer in zip(df_train.video_name, df_train.question, df_train.answer):\n",
        "    name = f'videos/{video_name}.mp4'\n",
        "    if os.path.exists(name):\n",
        "        data += [(name,f'Q: {question} A: {answer}')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bjj0uB4gccCS"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import os\n",
        "import PIL\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import transformers\n",
        "import more_itertools\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm\n",
        "from dataclasses import dataclass, field\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as TF\n",
        "import cv2\n",
        "import clip\n",
        "from PIL import Image\n",
        "import pickle\n",
        "from tqdm.contrib import tzip\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def image_grid(imgs, rows, cols):\n",
        "    pils = imgs\n",
        "    assert len(imgs) == rows*cols\n",
        "\n",
        "    w, h = imgs[0].size\n",
        "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
        "    grid_w, grid_h = grid.size\n",
        "\n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
        "    return grid\n",
        "\n",
        "def read_video(path, transform=None, frames_num=16, window=30):\n",
        "    frames = []\n",
        "    cap = cv2.VideoCapture(path)\n",
        "\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    N = length//(frames_num)\n",
        "    #print(length)\n",
        "    #counter =\n",
        "\n",
        "    current_frame = 1\n",
        "    for i in range(length):\n",
        "\n",
        "        #frameId = int(round(cap.get(current_frame)))\n",
        "        #print(current_frame)\n",
        "        ret, frame = cap.read(current_frame)\n",
        "\n",
        "        if ret and i==current_frame and len(frames)<frames_num:\n",
        "            size = 64, 64\n",
        "            frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "            frame.thumbnail(size, Image.ANTIALIAS)\n",
        "\n",
        "            frames.append(frame)\n",
        "            current_frame += N\n",
        "\n",
        "        #print(current_frame)\n",
        "        #cap.set(cv2.CAP_PROP_POS_FRAMES, current_frame)\n",
        "\n",
        "    cap.release()\n",
        "    #print(frames)\n",
        "    return frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uR5W2TTccdv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47e78ad0-bf47-4729-921a-2431b99f892e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 891M/891M [00:19<00:00, 47.6MiB/s]\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda')\n",
        "clip_model_type = \"ViT-L/14@336px\"\n",
        "\n",
        "out_path = f\"Features_train_full_ru.pkl\"\n",
        "video_path =  'videos'\n",
        "\n",
        "clip_model, preprocess = clip.load(clip_model_type, device=device, jit=False)\n",
        "\n",
        "# path_a = 'activitynet-qa/dataset/train_a.json'\n",
        "# path_q = 'activitynet-qa/dataset/train_q.json'\n",
        "# df_a = pd.read_json(path_a)\n",
        "# df_q = pd.read_json(path_q)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fBnjfERcjlp"
      },
      "outputs": [],
      "source": [
        "clip_model.to(device)\n",
        "None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71,
          "referenced_widgets": [
            "f84af7dfce4a4153a907e52b1a0ff3fe",
            "ac7509c0709a4834a6e8fc708a18be8c"
          ]
        },
        "id": "kwOK5wascju4",
        "outputId": "809def4a-2b3c-45ed-b429-cb242219bc5d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ac7509c0709a4834a6e8fc708a18be8c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/27570 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "all_embeddings = []\n",
        "all_captions = []\n",
        "i = 0\n",
        "\n",
        "for video_name, question, answer in tzip(df_train.video_name, df_train.question, df_train.answer):\n",
        "\n",
        "\n",
        "    name = f'{video_path}/{video_name}.mp4'\n",
        "\n",
        "    text = f'Q: {question} A: {answer}'\n",
        "    #print(name)\n",
        "    if os.path.exists(name):\n",
        "\n",
        "        video = read_video(path = name, frames_num=9)\n",
        "        if len(video)>1:\n",
        "            #print(len(video))\n",
        "            image = image_grid(video,3,3)\n",
        "\n",
        "            image = preprocess(image).unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                prefix = clip_model.encode_image(image).cpu()\n",
        "            #d[\"clip_embedding\"] = i\n",
        "            all_embeddings.append(prefix)\n",
        "            all_captions.append(text)\n",
        "\n",
        "with open(out_path, 'wb') as f:\n",
        "    pickle.dump({\"clip_embedding\": torch.cat(all_embeddings, dim=0), \"captions\": all_captions}, f)\n",
        "\n",
        "print('Done')\n",
        "print(\"%0d embeddings saved \" % len(all_embeddings))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5uF07hEqcltu"
      },
      "outputs": [],
      "source": [
        "\n",
        "import gc\n",
        "import io\n",
        "\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torchvision\n",
        "import transformers\n",
        "import more_itertools\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm\n",
        "from dataclasses import dataclass, field\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as TF\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import pickle\n",
        "import sys\n",
        "from tqdm.contrib import tzip\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as nnf\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import sys\n",
        "import argparse\n",
        "import json\n",
        "from typing import Tuple, Optional, Union\n",
        "from torch.cuda.amp import autocast\n",
        "\n",
        "from transformers.optimization import Adafactor, AdafactorSchedule\n",
        "import wandb\n",
        "import torch\n",
        "\n",
        "from torch.utils.checkpoint import checkpoint_sequential\n",
        "\n",
        "class ClipCocoDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data_path: str,  prefix_length= 50, gpt2_type = \"sberbank-ai/rugpt3large_based_on_gpt2\",\n",
        "                 normalize_prefix=False):\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_type)\n",
        "        self.prefix_length = prefix_length\n",
        "        self.normalize_prefix = normalize_prefix\n",
        "        with open(data_path, 'rb') as f:\n",
        "            all_data = pickle.load(f)\n",
        "        print(\"Data size is %0d\" % len(all_data[\"clip_embedding\"]))\n",
        "        sys.stdout.flush()\n",
        "        self.prefixes = all_data[\"clip_embedding\"]\n",
        "        captions_raw = all_data[\"captions\"]\n",
        "\n",
        "        #self.image_ids = [caption[\"image_id\"] for caption in captions_raw]\n",
        "\n",
        "        self.captions = captions_raw\n",
        "\n",
        "\n",
        "        self.captions_tokens = []\n",
        "        self.caption2embedding = []\n",
        "        max_seq_len = 0\n",
        "        i=0\n",
        "        for caption in tqdm(captions_raw):\n",
        "                self.captions_tokens.append(torch.tensor(self.tokenizer.encode(caption), dtype=torch.int64))\n",
        "                self.caption2embedding.append(self.prefixes[i])\n",
        "                i+=1\n",
        "                max_seq_len = max(max_seq_len, self.captions_tokens[-1].shape[0])\n",
        "            # self.max_seq_len = max_seq_len\n",
        "        #del self.captions_tokens\n",
        "        #del self.caption2embedding\n",
        "        #gc.collect()\n",
        "        #with open(f\"{data_path[:-4]}_tokens.pkl\", 'wb') as f:\n",
        "        #        pickle.dump([self.captions_tokens, self.caption2embedding, max_seq_len], f)\n",
        "\n",
        "\n",
        "\n",
        "        all_len = torch.tensor([len(self.captions_tokens[i]) for i in range(len(self))]).float()\n",
        "        self.max_seq_len = min(int(all_len.mean() + all_len.std() * 10), int(all_len.max()))\n",
        "\n",
        "    def pad_tokens(self, item: int):\n",
        "        tokens = self.captions_tokens[item]\n",
        "        padding = self.max_seq_len - tokens.shape[0]\n",
        "        if padding > 0:\n",
        "            tokens = torch.cat((tokens, torch.zeros(padding, dtype=torch.int64) - 1))\n",
        "            self.captions_tokens[item] = tokens\n",
        "        elif padding < 0:\n",
        "            tokens = tokens[:self.max_seq_len]\n",
        "            self.captions_tokens[item] = tokens\n",
        "        mask = tokens.ge(0)  # mask is zero where we out of sequence\n",
        "        tokens[~mask] = 0\n",
        "        mask = mask.float()\n",
        "        mask = torch.cat((torch.ones(self.prefix_length), mask), dim=0)  # adding prefix mask\n",
        "        return tokens, mask\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.captions_tokens)\n",
        "\n",
        "\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        tokens, mask = self.pad_tokens(item)\n",
        "        prefix = self.prefixes[item]\n",
        "        if self.normalize_prefix:\n",
        "            prefix = prefix.float()\n",
        "            prefix = prefix / prefix.norm(2, -1)\n",
        "        return tokens, mask, prefix\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6d-G11Bc-Lb",
        "outputId": "55b0f5df-292e-45cc-95bf-426cf7fac28f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data size is 27570\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 27570/27570 [00:03<00:00, 7264.12it/s]\n"
          ]
        }
      ],
      "source": [
        "dataset = ClipCocoDataset('Features_train_full_ru.pkl', prefix_length=50, normalize_prefix=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51d4XF65f7Eh",
        "outputId": "5b8bdf02-7349-4f92-dcb4-9b2de08754ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "Aborted!\n"
          ]
        }
      ],
      "source": [
        "! e461a6a3bca9f7cec3390a40dc10cdf576ce3252"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3N3MQ7afj8m"
      },
      "outputs": [],
      "source": [
        "\n",
        "wandb.init(project=\"clip_caption_video\")\n",
        "\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, sizes: Tuple[int, ...], bias=True, act=nn.Tanh):\n",
        "        super(MLP, self).__init__()\n",
        "        layers = []\n",
        "        for i in range(len(sizes) - 1):\n",
        "            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=bias))\n",
        "            if i < len(sizes) - 2:\n",
        "                layers.append(act())\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    #@autocast()\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "def freeze(\n",
        "    model,\n",
        "    freeze_emb=False,\n",
        "    freeze_ln=False,\n",
        "    freeze_attn=True,\n",
        "    freeze_ff=True,\n",
        "    freeze_other=True,\n",
        "):\n",
        "\n",
        "    for name, p in model.named_parameters():\n",
        "    # freeze all parameters except the layernorm and positional embeddings\n",
        "\n",
        "\n",
        "\n",
        "        name = name.lower()\n",
        "        if 'ln' in name or 'norm' in name:\n",
        "            p.requires_grad = not freeze_ln\n",
        "        elif 'embeddings' in name:\n",
        "            p.requires_grad = not freeze_emb\n",
        "        elif 'mlp' in name:\n",
        "            p.requires_grad = not freeze_ff\n",
        "        elif 'attn' in name:\n",
        "            p.requires_grad = not freeze_attn\n",
        "        else:\n",
        "            p.requires_grad = not freeze_other\n",
        "\n",
        "    return model\n",
        "\n",
        "class ClipCaptionModel(nn.Module):\n",
        "    def __init__(self, backbone, prefix_length: int, prefix_size: int = 768):\n",
        "          super(ClipCaptionModel, self).__init__()\n",
        "          self.prefix_length = prefix_length\n",
        "          \"\"\"\n",
        "          ru gpts shit\n",
        "\n",
        "          \"\"\"\n",
        "          self.gpt = GPT2LMHeadModel.from_pretrained(backbone)\n",
        "          #self.gpt = freeze(self.gpt)\n",
        "          self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n",
        "          self.clip_project = MLP((prefix_size, (self.gpt_embedding_size * prefix_length) // 2,\n",
        "                                  self.gpt_embedding_size * prefix_length))\n",
        "\n",
        "    def get_dummy_token(self, batch_size: int, device: torch.device) -> torch.Tensor:\n",
        "        return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)\n",
        "\n",
        "    # @autocast()\n",
        "    def forward(self, tokens: torch.Tensor, prefix: torch.Tensor, mask: Optional[torch.Tensor] = None,\n",
        "                labels: Optional[torch.Tensor] = None):\n",
        "\n",
        "        embedding_text = self.gpt.transformer.wte(tokens)\n",
        "        prefix_projections = self.clip_project(prefix).view(-1, self.prefix_length, self.gpt_embedding_size)\n",
        "\n",
        "        embedding_cat = torch.cat((prefix_projections, embedding_text), dim=1)\n",
        "        if labels is not None:\n",
        "            dummy_token = self.get_dummy_token(tokens.shape[0], tokens.device)\n",
        "            labels = torch.cat((dummy_token, tokens), dim=1)\n",
        "        out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=mask)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ClipCaptionPrefix(ClipCaptionModel):\n",
        "\n",
        "    def parameters(self, recurse: bool = True):\n",
        "        return self.clip_project.parameters()\n",
        "\n",
        "    def train(self, mode: bool = True):\n",
        "        super(ClipCaptionPrefix, self).train(mode)\n",
        "        self.gpt.eval()\n",
        "        return self\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train(dataset, model: ClipCaptionModel, args,\n",
        "          warmup_steps: int = 5000, output_dir: str = \".\", output_prefix: str = \"\"):\n",
        "\n",
        "    device = torch.device('cuda')# xm.xla_device()\n",
        "    #\n",
        "    batch_size = args.bs\n",
        "    epochs = args.epochs\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    model = freeze(model)\n",
        "    model.train()\n",
        "    optimizer = AdamW(model.parameters(), lr=args.lr,betas=(0.9, 0.995))\n",
        "    #optimizer = bnb.optim.Adam8bit(model.parameters(), lr=0.001, betas=(0.9, 0.995))\n",
        "    #optimizer = SM3(model.parameters(),lr=args.lr)\n",
        "    #Adafactor(model.parameters(),scale_parameter=True, relative_step=True, warmup_init=True, lr=None)\n",
        "\n",
        "    train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=epochs * len(train_dataloader)\n",
        "    )\n",
        "    #AdafactorSchedule(optimizer)#num_training_steps=epochs * len(train_dataloader\n",
        "    #save_config(args)\n",
        "    #print\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\">>> Training epoch {epoch}\")\n",
        "        sys.stdout.flush()\n",
        "        progress = tqdm(total=len(train_dataloader), desc=output_prefix)\n",
        "        step=0\n",
        "        for idx, (tokens, mask, prefix) in enumerate(train_dataloader):\n",
        "            model.zero_grad()\n",
        "            step+=1\n",
        "            tokens, mask, prefix = tokens.to(device), mask.to(device), prefix.to(device, dtype=torch.float32)\n",
        "\n",
        "            outputs = model(tokens, prefix, mask)\n",
        "            logits = outputs.logits[:, dataset.prefix_length - 1: -1]\n",
        "\n",
        "            loss = nnf.cross_entropy(logits.reshape(-1, logits.shape[-1]), tokens.flatten(), ignore_index=0)\n",
        "\n",
        "            segments = 2\n",
        "\n",
        "\n",
        "            #out = checkpoint_sequential(modules, segments, input_var)\n",
        "\n",
        "            # backpropagate\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            #optimizer.zero_grad()\n",
        "            progress.set_postfix({\"loss\": loss.item()})\n",
        "\n",
        "            clipping_value = 0.5 # arbitrary value of your choosing\n",
        "            #torch.nn.utils.clip_grad_norm(model.parameters(), clipping_value)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            wandb.log({\"loss\":  loss.item()})\n",
        "\n",
        "            progress.update()\n",
        "\n",
        "\n",
        "            del tokens\n",
        "            del mask\n",
        "            del prefix\n",
        "            torch.clear_autocast_cache()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            if (idx + 1) % 7000 == 0:\n",
        "                torch.save(\n",
        "                    model.state_dict(),\n",
        "\n",
        "                    os.path.join(output_dir, f\"{output_prefix}_latest.pt\"),\n",
        "                )\n",
        "        progress.close()\n",
        "        if epoch % args.save_every ==0:\n",
        "            torch.save(\n",
        "                model.state_dict(),\n",
        "                os.path.join(output_dir, f\"{output_prefix}-{epoch:03d}.pt\"),\n",
        "            )\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "class Args():\n",
        "    def __init__(self):\n",
        "        self.backbone = 'sberbank-ai/rugpt3small_based_on_gpt2'\n",
        "        self.data = 'Features_train_full_ru.pkl'\n",
        "        self.out_dir = 'checkpoints_larger'\n",
        "        self.prefix = 'prefix_1'\n",
        "        self.epochs = 10\n",
        "        self.save_every = 1\n",
        "        self.prefix_length = 50\n",
        "        self.bs = 20\n",
        "        self.only_prefix = False\n",
        "        self.lr = 5e-5\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "\n",
        "    args = Args()\n",
        "    wandb.config = {\n",
        "      \"learning_rate\": args.lr,\n",
        "      \"epochs\": args.epochs,\n",
        "      \"batch_size\": args.bs\n",
        "    }\n",
        "\n",
        "    prefix_length = args.prefix_length\n",
        "\n",
        "    dataset = ClipCocoDataset(args.data, prefix_length)\n",
        "\n",
        "\n",
        "    #model_path = 'prefix_1-003.pt'\n",
        "    model = ClipCaptionModel(backbone = 'sberbank-ai/rugpt3large_based_on_gpt2', prefix_length = 50)\n",
        "    # model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
        "    print(\"Train both prefix and GPT\")\n",
        "    sys.stdout.flush()\n",
        "    train(dataset, model, args, output_dir=args.out_dir, output_prefix=args.prefix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqnMOi63Hkbs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9cI3gxHhDY6"
      },
      "outputs": [],
      "source": [
        "!rm -r checkpoints*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qT6UdHwPhJGa"
      },
      "outputs": [],
      "source": [
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyO_SyvJAepw"
      },
      "outputs": [],
      "source": [
        "!cp /content/checkpoints/prefix_1-007.pt /content/drive/MyDrive/4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_hej1zYCopu"
      },
      "outputs": [],
      "source": [
        "!cp /content/drive/MyDrive/4/prefix_1-007.pt ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doKt6VnHAMc5"
      },
      "outputs": [],
      "source": [
        "!pip install tg-logger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxwaqhsMHmM5"
      },
      "outputs": [],
      "source": [
        "import clip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqNobiKVGTVi"
      },
      "outputs": [],
      "source": [
        "device = 'cuda'\n",
        "clip_model, preprocess = clip.load(\"ViT-L/14@336px\", device=device, jit=False)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('sberbank-ai/rugpt3large_based_on_gpt2')\n",
        "prefix_length= 50\n",
        "model_path = 'prefix_1-007.pt'\n",
        "model = ClipCaptionModel(backbone = 'gpt2', prefix_length = 50)\n",
        "model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
        "model.to(device)\n",
        "None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnGs2uMtH3IW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cl1Kh03qhq3-"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import os\n",
        "import PIL\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import transformers\n",
        "import more_itertools\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "#from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset\n",
        "#from tqdm import tqdm\n",
        "from dataclasses import dataclass, field\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as TF\n",
        "import cv2\n",
        "from PIL import Image\n",
        "def image_grid(imgs, rows, cols):\n",
        "    pils = imgs\n",
        "\n",
        "    assert len(imgs) == rows*cols\n",
        "\n",
        "    w, h = imgs[0].size\n",
        "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
        "    grid_w, grid_h = grid.size\n",
        "\n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
        "    return grid\n",
        "def read_video(path, transform=None, frames_num=9, window=30):\n",
        "    frames = []\n",
        "    cap = cv2.VideoCapture(path)\n",
        "\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    N = length//(frames_num)\n",
        "    #print(length)\n",
        "    #counter =\n",
        "\n",
        "    current_frame = 1\n",
        "    for i in range(length):\n",
        "\n",
        "        #frameId = int(round(cap.get(current_frame)))\n",
        "        #print(current_frame)\n",
        "        ret, frame = cap.read(current_frame)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        if ret and i==current_frame and len(frames)<frames_num:\n",
        "            size = 193, 193\n",
        "            frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "            frame.thumbnail(size, Image.ANTIALIAS)\n",
        "\n",
        "            frames.append(frame)\n",
        "            current_frame += N\n",
        "\n",
        "\n",
        "        #print(current_frame)\n",
        "        #cap.set(cv2.CAP_PROP_POS_FRAMES, current_frame)\n",
        "\n",
        "\n",
        "    cap.release()\n",
        "    #print(frames)\n",
        "    return frames\n",
        "\n",
        "\n",
        "\n",
        "def filter_ngrams(output_text):\n",
        "    a_pos = output_text.find(' A:')\n",
        "    sec_a_pos = output_text.find(' A:', a_pos + 1)\n",
        "\n",
        "    return output_text[:sec_a_pos]\n",
        "\n",
        "def generate2(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        tokens=None,\n",
        "        prompt='',\n",
        "        embed=None,\n",
        "        entry_count=1,\n",
        "        entry_length=67,  # maximum number of words\n",
        "        top_p=0.98,\n",
        "        temperature=1.,\n",
        "        stop_token = '.',\n",
        "):\n",
        "    model.eval()\n",
        "    generated_num = 0\n",
        "    generated_list = []\n",
        "    stop_token_index = tokenizer.encode(stop_token)[0]\n",
        "    filter_value = -float(\"Inf\")\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for entry_idx in range(entry_count):\n",
        "            if not tokens:\n",
        "                    tokens = torch.tensor(tokenizer.encode(prompt))\n",
        "                    #print('tokens',tokens)\n",
        "                    tokens = tokens.unsqueeze(0).to(device)\n",
        "\n",
        "            emb_tokens = model.gpt.transformer.wte(tokens)\n",
        "\n",
        "            if embed is not None:\n",
        "                generated = torch.cat((embed, emb_tokens), dim=1)\n",
        "            else:\n",
        "                generated = emb_tokens\n",
        "\n",
        "            for i in range(entry_length):\n",
        "\n",
        "                outputs = model.gpt(inputs_embeds=generated)\n",
        "                logits = outputs.logits\n",
        "                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
        "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "                cumulative_probs = torch.cumsum(nnf.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "                sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n",
        "                                                    ..., :-1\n",
        "                                                    ].clone()\n",
        "                sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "                logits[:, indices_to_remove] = filter_value\n",
        "                #\n",
        "                top_k = 2000\n",
        "                top_p = 0.98\n",
        "                #print(logits)\n",
        "                #next_token = transformers.top_k_top_p_filtering(logits.to(torch.int64).unsqueeze(0), top_k=top_k, top_p=top_p)\n",
        "                next_token = torch.argmax(logits, -1).unsqueeze(0)\n",
        "                next_token_embed = model.gpt.transformer.wte(next_token)\n",
        "\n",
        "                if tokens is None:\n",
        "                    tokens = next_token\n",
        "                else:\n",
        "                    tokens = torch.cat((tokens, next_token), dim=1)\n",
        "                generated = torch.cat((generated, next_token_embed), dim=1)\n",
        "\n",
        "                if stop_token_index == next_token.item():\n",
        "                    break\n",
        "\n",
        "            output_list = list(tokens.squeeze().cpu().numpy())\n",
        "\n",
        "            output_text = tokenizer.decode(output_list)\n",
        "            output_text = filter_ngrams(output_text)\n",
        "            generated_list.append(output_text)\n",
        "\n",
        "    return generated_list[0]\n",
        "#from tqdm import tqdm, trange\n",
        "\n",
        "\n",
        "def _to_caption(pil_image,prompt=''):\n",
        "    device = 'cuda:0'\n",
        "    image = preprocess(pil_image).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "\n",
        "        prefix = clip_model.encode_image(image).to(device, dtype=torch.float32)\n",
        "        prefix_embed = model.clip_project(prefix).reshape(1, prefix_length, -1)\n",
        "        if prompt:\n",
        "            generated_text_prefix = generate2(model, tokenizer, prompt=prompt, embed=prefix_embed)\n",
        "        else:\n",
        "            generated_text_prefix = generate2(model, tokenizer, embed=prefix_embed)\n",
        "    return generated_text_prefix.replace('\\n',' ').replace('\\xa0','')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlhoXxtbALdK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekZVTiP0IC9r"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wV7qGc8ODxEF"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import datetime\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import json\n",
        "from torchvision.transforms import functional as TF\n",
        "\n",
        "\n",
        "import pytz\n",
        "import argparse\n",
        "\n",
        "\n",
        "\n",
        "from telebot import types\n",
        "import tg_logger\n",
        "import logging\n",
        "import telebot\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import json\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def unique_list(l):\n",
        "    ulist = []\n",
        "    [ulist.append(x) for x in l if x not in ulist]\n",
        "    return ulist\n",
        "\n",
        "\n",
        "boot_time = time.time()\n",
        "boot_date = datetime.datetime.now(tz=pytz.timezone(\"Europe/Moscow\"))\n",
        "\n",
        "# ------------- flask config -------------\n",
        "\n",
        "\n",
        "# ------------- bot config -------------\n",
        "WEBHOOK_TOKEN = 'aa'\n",
        "BOT_TOKEN = '5676745030:AAEcXUG-wF-IMBTbDFl11ZhXwlkVKqOvaMM'\n",
        "bot = telebot.TeleBot(BOT_TOKEN)\n",
        "\n",
        "# ------------- log ---------------\n",
        "users = ['241154130']\n",
        "\n",
        "alpha_logger = logging.getLogger()\n",
        "alpha_logger.setLevel(logging.INFO)\n",
        "tg_logger.setup(alpha_logger, token=\"1227347441:AAEnih283opCWcQLFcbghBXc_t1tIp64QXA\", users=users)\n",
        "\n",
        "logger = logging.getLogger(\"tg-bot-tti\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import re\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "@bot.message_handler(commands=['help', 'start'])\n",
        "def say_welcome(message):\n",
        "    '''Displaying the bot's start interface'''\n",
        "\n",
        "    logger.info(f'</code>@{message.from_user.username}<code> ({message.chat.id}) used /start or /help')\n",
        "    bot.send_message(message.chat.id,\n",
        "                     \"\"\" Text2Image  generate faces here \"\"\",\n",
        "                     parse_mode='html')\n",
        "\n",
        "#from PIL import Image\n",
        "#@bot.message_handler(content_types=['video'])\n",
        "#def get_file(message):\n",
        "#    file_name = message.json['video']['file_name']\n",
        "#    file_info = bot.get_file(message.video.file_id)\n",
        "#    with open(file_name, \"wb\") as f:\n",
        "#        file_content = bot.download_file(file_info.file_path)\n",
        "#        f.write(file_content)\n",
        "#   bot.reply_to(message, f\"OK. Сохранил {file_name}\")\n",
        "\n",
        "\n",
        "@bot.message_handler(content_types=['video'])\n",
        "def photo(message):\n",
        "    q = message.caption\n",
        "\n",
        "    logger.info(f'{message.from_user.username} {q}')\n",
        "    file_name = message.json['video']['file_name']\n",
        "    file_info = bot.get_file(message.video.file_id)\n",
        "    with open(file_name, \"wb\") as f:\n",
        "        file_content = bot.download_file(file_info.file_path)\n",
        "\n",
        "        f.write(file_content)\n",
        "    try:\n",
        "        path = file_name\n",
        "\n",
        "\n",
        "        video = read_video(path = path, transform = None,frames_num=4)\n",
        "        i = image_grid (video,2,2)\n",
        "        ans = _to_caption(i, prompt=f'Q:{q} A:')\n",
        "\n",
        "        #image = PIL.Image.open(\"image.jpg\")\n",
        "        #ans = _to_caption(image).replace('<|endoftext|>','')\n",
        "        #ans = ' '.join(unique_list(ans.split()))\n",
        "        #print(f'{message.from_user.username} {ans}')\n",
        "        logger.info(f'{message.from_user.username} {ans}')\n",
        "        #bot.send_message(message.chat.id,ans)\n",
        "        #class_ = clf(image)[0]\n",
        "        bot.send_message(message.chat.id, f'{ans.split(\"A:\")[1]}' )\n",
        "    except Exception as e:\n",
        "         bot.send_message(message.chat.id, e )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "\n",
        "  bot.polling(none_stop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijbsQSPJFINb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuClass": "premium"
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}