{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31da2978",
   "metadata": {},
   "source": [
    "# –®–∞–±–ª–æ–Ω-–±–µ–π–∑–ª–∞–π–Ω –¥–ª—è –æ–ª–∏–º–ø–∏–∞–¥—ã –ø–æ ML / DS (—Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫)\n",
    "\n",
    "–≠—Ç–æ—Ç –±–ª–æ–∫–Ω–æ—Ç ‚Äî **—Å–∫–µ–ª–µ—Ç —Ä–µ—à–µ–Ω–∏—è** –ø–æ–¥ —á–µ—Ç—ã—Ä–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏ –æ–ª–∏–º–ø–∏–∞–¥—ã:\n",
    "\n",
    "1. üìñ –ó–∞–¥–∞—á–∞ 1: –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ / LLM (—Ç–µ–∫—Å—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º)\n",
    "2. üéß –ó–∞–¥–∞—á–∞ 2A: —Ä–∞–±–æ—Ç–∞ —Å–æ –∑–≤—É–∫–æ–º (–∞—É–¥–∏–æ‚Äë–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è / —Ä–µ–≥—Ä–µ—Å—Å–∏—è)\n",
    "3. ‚≠ê –ó–∞–¥–∞—á–∞ 2B: —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞\n",
    "4. üåç –ó–∞–¥–∞—á–∞ 2C: —Ä–∞–±–æ—Ç–∞ —Å –≥–µ–æ–¥–∞–Ω–Ω—ã–º–∏\n",
    "\n",
    "–ò–¥–µ—è: —Ç—ã –Ω–∞ –º–µ—Å—Ç–µ **–ø–æ–¥—Å—Ç–∞–≤–ª—è–µ—à—å —Å–≤–æ–∏ —Ñ–∞–π–ª—ã –∏ –Ω–∞–∑–≤–∞–Ω–∏—è —Å—Ç–æ–ª–±—Ü–æ–≤**, –∞ –≤—Å—è –±–∞–∑–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —É–∂–µ –≥–æ—Ç–æ–≤–∞: –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö, —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —Ç—Ä–µ–πŒΩ/–≤–∞–ª–∏–¥–∞—Ü–∏—é, –ø—Ä–æ—Å—Ç—ã–µ –º–æ–¥–µ–ª–∏, –∏–Ω—Ñ–µ—Ä–µ–Ω—Å –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–∞–±–º–∏—Ç–∞.\n",
    "\n",
    "> ‚ö†Ô∏è –í–∞–∂–Ω–æ: –≤ —Å—Ä–µ–¥–µ –æ–ª–∏–º–ø–∏–∞–¥—ã –º–æ–∂–µ—Ç **–Ω–µ –±—ã—Ç—å –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞**, –∞ –Ω–∞–±–æ—Ä –±–∏–±–ª–∏–æ—Ç–µ–∫ –º–æ–∂–µ—Ç –æ—Ç–ª–∏—á–∞—Ç—å—Å—è. –Ø –æ—Ç–º–µ—á—É –º–µ—Å—Ç–∞, –≥–¥–µ –Ω—É–∂–Ω–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å, —á—Ç–æ –≤—Å—ë —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ, –∏–ª–∏ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –∑–∞–º–µ–Ω–∏—Ç—å –º–æ–¥–µ–ª—å/–±–∏–±–ª–∏–æ—Ç–µ–∫—É.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38eba4c4",
   "metadata": {},
   "source": [
    "## 0. –û–±—â–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏\n",
    "\n",
    "–ó–∞–¥–∞—ë–º —Å–∏–¥—ã, –ø–æ–¥–∫–ª—é—á–∞–µ–º –±–∞–∑–æ–≤—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏, –ø—Ä–æ–≤–µ—Ä—è–µ–º GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "967101a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–í–µ—Ä—Å–∏—è Python: 3.13.1 (tags/v3.13.1:0671451, Dec  3 2024, 19:06:28) [MSC v.1942 64 bit (AMD64)]\n",
      "–í–µ—Ä—Å–∏—è PyTorch: 2.7.1+cu118\n",
      "CUDA –¥–æ—Å—Ç—É–ø–Ω–∞: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "print(\"–í–µ—Ä—Å–∏—è Python:\", os.sys.version)\n",
    "print(\"–í–µ—Ä—Å–∏—è PyTorch:\", torch.__version__)\n",
    "print(\"CUDA –¥–æ—Å—Ç—É–ø–Ω–∞:\", torch.cuda.is_available())\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda323a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac218ff8",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. –ó–∞–¥–∞—á–∞ 1: –ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å / LLM (—Ç–µ–∫—Å—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º)\n",
    "\n",
    "–ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, —á—Ç–æ —É –Ω–∞—Å –µ—Å—Ç—å –∑–∞–¥–∞—á–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –∏–ª–∏ –¥–æ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –ø–æ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–æ–≤.\n",
    "\n",
    "–ù–∞–ø—Ä–∏–º–µ—Ä, —Ñ–∞–π–ª `train_llm.csv` –º–æ–∂–µ—Ç –∏–º–µ—Ç—å –∫–æ–ª–æ–Ω–∫–∏:\n",
    "- `prompt` ‚Äî –≤—Ö–æ–¥ (–≤–æ–ø—Ä–æ—Å, –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è)\n",
    "- `response` ‚Äî —Ü–µ–ª–µ–≤–æ–π –æ—Ç–≤–µ—Ç\n",
    "- (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è\n",
    "\n",
    "–ò `test_llm.csv` —Å –∫–æ–ª–æ–Ω–∫–æ–π `prompt`, –¥–ª—è –∫–æ—Ç–æ—Ä–æ–π –Ω—É–∂–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å `response`.\n",
    "\n",
    "–ù–∏–∂–µ ‚Äî **–±–∞–∑–æ–≤—ã–π –ø–∞–π–ø–ª–∞–π–Ω** –Ω–∞ `transformers`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97ddb61",
   "metadata": {},
   "source": [
    "### 1.1 –£—Å—Ç–∞–Ω–æ–≤–∫–∞ / –∏–º–ø–æ—Ä—Ç LLM-–±–∏–±–ª–∏–æ—Ç–µ–∫\n",
    "\n",
    "> ‚ö†Ô∏è –ï—Å–ª–∏ –≤ —Å—Ä–µ–¥–µ —É–∂–µ –µ—Å—Ç—å `transformers`, `datasets`, —ç—Ç–æ—Ç —à–∞–≥ –º–æ–∂–Ω–æ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å.  \n",
    "> ‚ö†Ô∏è –ï—Å–ª–∏ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞ –Ω–µ—Ç ‚Äî –º–æ–¥–µ–ª—å –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç –∑–∞—Ä–∞–Ω–µ–µ —Å–∫–∞—á–∞—Ç—å –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–æ–∫–∞–ª—å–Ω—É—é.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c211618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ú–æ–¥–µ–ª—å: Vikhrmodels/Vikhr-Gemma-2B-instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  7.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# –ï—Å–ª–∏ –Ω—É–∂–Ω–æ ‚Äî —Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π —Å—Ç—Ä–æ–∫–∏ –Ω–∏–∂–µ (–∏ —É–±–µ–¥–∏—Å—å, —á—Ç–æ –µ—Å—Ç—å –∏–Ω—Ç–µ—Ä–Ω–µ—Ç)\n",
    "# !pip install transformers datasets accelerate sentencepiece\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from datasets import Dataset\n",
    "\n",
    "MODEL_NAME = \"Vikhrmodels/Vikhr-Gemma-2B-instruct\"  # –ú–æ–∂–Ω–æ –ø–æ–º–µ–Ω—è—Ç—å –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏\n",
    "print(\"–ú–æ–¥–µ–ª—å:\", MODEL_NAME)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "\n",
    "# –î–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Ä—É—Å—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π –º–æ–∂–µ—Ç –ø–æ–Ω–∞–¥–æ–±–∏—Ç—å—Å—è:\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb2e9e1",
   "metadata": {},
   "source": [
    "### 1.2 –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è LLM-–∑–∞–¥–∞—á–∏\n",
    "\n",
    "–û–∂–∏–¥–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç:\n",
    "\n",
    "```text\n",
    "train_llm.csv: prompt, response\n",
    "test_llm.csv:  prompt\n",
    "```\n",
    "\n",
    "–¢—ã –º–æ–∂–µ—à—å –ø–æ–º–µ–Ω—è—Ç—å –ø—É—Ç–∏ –∏ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫ –ø–æ–¥ —Ä–µ–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b93410",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"./data\")  # –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø–æ–º–µ–Ω—è–π\n",
    "train_llm_path = DATA_DIR / \"train_llm.csv\"\n",
    "test_llm_path = DATA_DIR / \"test_llm.csv\"\n",
    "\n",
    "train_llm = pd.read_csv(train_llm_path)\n",
    "test_llm = pd.read_csv(test_llm_path)\n",
    "\n",
    "print(\"train_llm shape:\", train_llm.shape)\n",
    "print(\"test_llm shape:\", test_llm.shape)\n",
    "train_llm.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4de779",
   "metadata": {},
   "source": [
    "### 1.3 –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
    "\n",
    "–ú—ã —Å–∫–ª–µ–∏–º `prompt` –∏ `response` –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É (—Ñ–æ—Ä–º–∞—Ç –º–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –ø–æ–¥ —É—Å–ª–æ–≤–∏–µ –∑–∞–¥–∞—á–∏)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d0d69ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated f-string literal (detected at line 4) (1628311148.py, line 4)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mreturn f\"–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è: {example['prompt']}\u001b[39m\n           ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unterminated f-string literal (detected at line 4)\n"
     ]
    }
   ],
   "source": [
    "def build_instruction(example):\n",
    "    # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –∑–∞—à–∏—Ç—å —Ñ–æ—Ä–º–∞—Ç –ø–æ–¥—Å–∫–∞–∑–∫–∏ –ø–æ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –∑–∞–¥–∞—á—É –æ–ª–∏–º–ø–∏–∞–¥—ã\n",
    "    # –ù–∞–ø—Ä–∏–º–µ—Ä: \"–í–æ–ø—Ä–æ—Å: {prompt}\\n–û—Ç–≤–µ—Ç: {response}\"\n",
    "    return f\"–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è: {example['prompt']}\n",
    "–û—Ç–≤–µ—Ç: {example['response']}\"\n",
    "\n",
    "train_texts = [build_instruction(row) for _, row in train_llm.iterrows()]\n",
    "\n",
    "train_dataset = Dataset.from_dict({\"text\": train_texts})\n",
    "\n",
    "def tokenize_function(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9699ec",
   "metadata": {},
   "source": [
    "### 1.4 –û–±—É—á–µ–Ω–∏–µ LLM (–¥–æ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ —Å–≤–æ–∏—Ö –¥–∞–Ω–Ω—ã—Ö)\n",
    "\n",
    "–î–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏ –≤ –±–µ–π–∑–ª–∞–π–Ω–µ –≤–æ–∑—å–º—ë–º **–º–∞–ª–µ–Ω—å–∫–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö** –∏ **–Ω–µ–±–æ–ª—å—à–æ–π batch size**. –ù–∞ –æ–ª–∏–º–ø–∏–∞–¥–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–∂–Ω–æ –±—É–¥–µ—Ç —É–≤–µ–ª–∏—á–∏—Ç—å.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0776411",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./llm_russian_baseline\"\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,          # —É–≤–µ–ª–∏—á–∏–≤–∞–π –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=50,\n",
    "    logging_steps=50,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b648a0",
   "metadata": {},
   "source": [
    "### 1.5 –ò–Ω—Ñ–µ—Ä–µ–Ω—Å: –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∞\n",
    "\n",
    "–ë—É–¥–µ–º –ø–æ `prompt` —Å—Ç—Ä–æ–∏—Ç—å —Å—Ç—Ä–æ–∫—É-–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b055363d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "def build_prompt_for_inference(prompt_text: str) -> str:\n",
    "    # –§–æ—Ä–º–∞—Ç –¥–æ–ª–∂–µ–Ω —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å —Ñ–æ—Ä–º–∞—Ç–æ–º –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏\n",
    "    return f\"–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è: {prompt_text}\n",
    "–û—Ç–≤–µ—Ç:\"\n",
    "\n",
    "def generate_answer(prompt_text: str, max_new_tokens: int = 64) -> str:\n",
    "    input_text = build_prompt_for_inference(prompt_text)\n",
    "    inputs = tokenizer(\n",
    "        input_text,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.8,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    full_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    # –û–±—Ä–µ–∑–∞–µ–º –≤—Å—ë –¥–æ \"–û—Ç–≤–µ—Ç:\" –∏ –±–µ—Ä—ë–º —Ö–≤–æ—Å—Ç\n",
    "    if \"–û—Ç–≤–µ—Ç:\" in full_text:\n",
    "        answer_part = full_text.split(\"–û—Ç–≤–µ—Ç:\", 1)[1]\n",
    "    else:\n",
    "        answer_part = full_text\n",
    "    return answer_part.strip()\n",
    "\n",
    "\n",
    "test_predictions = []\n",
    "for i, row in test_llm.iterrows():\n",
    "    ans = generate_answer(row[\"prompt\"])\n",
    "    test_predictions.append(ans)\n",
    "    if i < 3:\n",
    "        print(\"PROMPT:\", row[\"prompt\"])\n",
    "        print(\"ANSWER:\", ans)\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "submission_llm = test_llm.copy()\n",
    "submission_llm[\"response\"] = test_predictions\n",
    "\n",
    "# –ü—É—Ç—å –∏ —Ñ–æ—Ä–º–∞—Ç —Å–∞–±–º–∏—Ç–∞ –º–æ–∂–µ—à—å –ø–æ–º–µ–Ω—è—Ç—å –ø–æ–¥ —É—Å–ª–æ–≤–∏—è –æ–ª–∏–º–ø–∏–∞–¥—ã\n",
    "submission_llm_path = Path(\"./submission_llm.csv\")\n",
    "submission_llm.to_csv(submission_llm_path, index=False)\n",
    "submission_llm_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d7c62e",
   "metadata": {},
   "source": [
    "---\n",
    "## 2A. –ó–∞–¥–∞—á–∞: —Ä–∞–±–æ—Ç–∞ —Å–æ –∑–≤—É–∫–æ–º (–∞—É–¥–∏–æ)\n",
    "\n",
    "–ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, —á—Ç–æ –µ—Å—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∞—É–¥–∏–æ‚Äë—Ñ–∞–π–ª–æ–≤.  \n",
    "–ù–∞–ø—Ä–∏–º–µ—Ä, `train_audio.csv`:\n",
    "\n",
    "- `path` ‚Äî –ø—É—Ç—å –¥–æ .wav —Ñ–∞–π–ª–∞\n",
    "- `label` ‚Äî –º–µ—Ç–∫–∞ –∫–ª–∞—Å—Å–∞\n",
    "\n",
    "–ò `test_audio.csv` —Å –∫–æ–ª–æ–Ω–∫–æ–π `path`.\n",
    "\n",
    "–ë–µ–π–∑–ª–∞–π–Ω: –ø–æ—Å—á–∏—Ç–∞–µ–º **–º–µ–ª-—Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º—É** –∏ –æ–±—É—á–∏–º –ø—Ä–æ—Å—Ç–æ–π CNN/MLP –≤ PyTorch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db52a46",
   "metadata": {},
   "source": [
    "### 2A.1 –£—Å—Ç–∞–Ω–æ–≤–∫–∞ / –∏–º–ø–æ—Ä—Ç –∞—É–¥–∏–æ-–±–∏–±–ª–∏–æ—Ç–µ–∫\n",
    "\n",
    "–ë—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `torchaudio` (—á–∞—Å—Ç–æ –∏–¥—ë—Ç –≤–º–µ—Å—Ç–µ —Å PyTorch).  \n",
    "–ï—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç ‚Äî –º–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ `librosa` (–Ω–æ —Ç–∞–º –º–æ–∂–µ—Ç –ø–æ–Ω–∞–¥–æ–±–∏—Ç—å—Å—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39df1fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import torchaudio\n",
    "    print(\"torchaudio –≤–µ—Ä—Å–∏—è:\", torchaudio.__version__)\n",
    "except ImportError as e:\n",
    "    print(\"torchaudio –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —É—Å—Ç–∞–Ω–æ–≤–∏ –∏–ª–∏ –∑–∞–º–µ–Ω–∏ –Ω–∞ librosa.\")\n",
    "    raise e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c9ed72",
   "metadata": {},
   "source": [
    "### 2A.2 –ó–∞–≥—Ä—É–∑–∫–∞ CSV –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ec0241",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_audio_path = DATA_DIR / \"train_audio.csv\"\n",
    "test_audio_path = DATA_DIR / \"test_audio.csv\"\n",
    "\n",
    "train_audio = pd.read_csv(train_audio_path)\n",
    "test_audio = pd.read_csv(test_audio_path)\n",
    "\n",
    "print(train_audio.head())\n",
    "print(test_audio.head())\n",
    "\n",
    "label2id = {lbl: i for i, lbl in enumerate(sorted(train_audio[\"label\"].unique()))}\n",
    "id2label = {i: lbl for lbl, i in label2id.items()}\n",
    "\n",
    "num_classes = len(label2id)\n",
    "num_classes, label2id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f21af9f",
   "metadata": {},
   "source": [
    "### 2A.3 PyTorch Dataset –¥–ª—è –∞—É–¥–∏–æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c7f595",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, df, data_dir: Path, label2id=None, train: bool = True, sample_rate: int = 16000):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.data_dir = data_dir\n",
    "        self.train = train\n",
    "        self.label2id = label2id\n",
    "        self.sample_rate = sample_rate\n",
    "\n",
    "        self.resampler = torchaudio.transforms.Resample(orig_freq=None, new_freq=sample_rate)\n",
    "        self.melspec = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_fft=1024,\n",
    "            hop_length=256,\n",
    "            n_mels=64\n",
    "        )\n",
    "        self.ampl2db = torchaudio.transforms.AmplitudeToDB()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        wav_path = self.data_dir / row[\"path\"]\n",
    "\n",
    "        waveform, sr = torchaudio.load(str(wav_path))\n",
    "        if sr != self.sample_rate:\n",
    "            waveform = self.resampler(waveform)\n",
    "\n",
    "        # –ú–æ–Ω–æ\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "        melspec = self.melspec(waveform)  # (1, n_mels, time)\n",
    "        melspec_db = self.ampl2db(melspec)\n",
    "        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "        melspec_db = (melspec_db - melspec_db.mean()) / (melspec_db.std() + 1e-6)\n",
    "\n",
    "        if self.train and self.label2id is not None:\n",
    "            label = self.label2id[row[\"label\"]]\n",
    "            return melspec_db, label\n",
    "        else:\n",
    "            return melspec_db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d6074a",
   "metadata": {},
   "source": [
    "### 2A.4 –ü—Ä–æ—Å—Ç–∞—è CNN‚Äë–º–æ–¥–µ–ª—å –¥–ª—è –º–µ–ª-—Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20e97ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAudioCNN(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((8, 8))\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, 1, n_mels, T)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9166ff",
   "metadata": {},
   "source": [
    "### 2A.5 –û–±—É—á–µ–Ω–∏–µ –∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81818af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, val_df = train_test_split(train_audio, test_size=0.2, stratify=train_audio[\"label\"], random_state=42)\n",
    "\n",
    "train_ds = AudioDataset(train_df, DATA_DIR, label2id=label2id, train=True)\n",
    "val_ds = AudioDataset(val_df, DATA_DIR, label2id=label2id, train=True)\n",
    "test_ds = AudioDataset(test_audio, DATA_DIR, label2id=None, train=False)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=32, shuffle=False)\n",
    "\n",
    "audio_model = SimpleAudioCNN(num_classes=num_classes).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(audio_model.parameters(), lr=1e-3)\n",
    "\n",
    "def train_epoch(model, loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_count = 0\n",
    "    for x, y in loader:\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        total_correct += (preds == y).sum().item()\n",
    "        total_count += x.size(0)\n",
    "    return total_loss / total_count, total_correct / total_count\n",
    "\n",
    "def eval_epoch(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_count = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            total_correct += (preds == y).sum().item()\n",
    "            total_count += x.size(0)\n",
    "    return total_loss / total_count, total_correct / total_count\n",
    "\n",
    "EPOCHS = 3  # —É–≤–µ–ª–∏—á–∏–≤–∞–π –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    tr_loss, tr_acc = train_epoch(audio_model, train_loader)\n",
    "    val_loss, val_acc = eval_epoch(audio_model, val_loader)\n",
    "    print(f\"Epoch {epoch}: train_loss={tr_loss:.4f}, train_acc={tr_acc:.4f}, val_loss={val_loss:.4f}, val_acc={val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e6ffff",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_model.eval()\n",
    "test_preds_ids = []\n",
    "with torch.no_grad():\n",
    "    for x in test_loader:\n",
    "        x = x.to(DEVICE)\n",
    "        logits = audio_model(x)\n",
    "        preds = logits.argmax(dim=1).cpu().numpy().tolist()\n",
    "        test_preds_ids.extend(preds)\n",
    "\n",
    "test_labels = [id2label[i] for i in test_preds_ids]\n",
    "\n",
    "submission_audio = test_audio.copy()\n",
    "submission_audio[\"label\"] = test_labels\n",
    "submission_audio_path = Path(\"./submission_audio.csv\")\n",
    "submission_audio.to_csv(submission_audio_path, index=False)\n",
    "submission_audio_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3be553",
   "metadata": {},
   "source": [
    "---\n",
    "## 2B. –ó–∞–¥–∞—á–∞: —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞\n",
    "\n",
    "–ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π —Ñ–æ—Ä–º–∞—Ç: –µ—Å—Ç—å —Ç–∞–±–ª–∏—Ü–∞ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å –æ–±—ä–µ–∫—Ç–∞–º–∏.\n",
    "\n",
    "–ù–∞–ø—Ä–∏–º–µ—Ä, `train_rec.csv`:\n",
    "\n",
    "- `user_id`\n",
    "- `item_id`\n",
    "- `rating` –∏–ª–∏ `target` (–º–æ–∂–µ—Ç –±—ã—Ç—å 0/1 –∏–ª–∏ –≤–µ—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ —á–∏—Å–ª–æ)\n",
    "\n",
    "–ò `test_rec.csv` —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ `user_id`, `item_id`, –≥–¥–µ –Ω—É–∂–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å `rating` / –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å.\n",
    "\n",
    "–ó–¥–µ—Å—å —Å–¥–µ–ª–∞–µ–º **–ø—Ä–æ—Å—Ç–æ–π –±–µ–π–∑–ª–∞–π–Ω**:\n",
    "1. –ü–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å –æ–±—ä–µ–∫—Ç–∞ (item bias)\n",
    "2. –ü—Ä–∏ –∂–µ–ª–∞–Ω–∏–∏ ‚Äî –º–∞—Ç—Ä–∏—á–Ω–∞—è —Ñ–∞–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –Ω–∞ PyTorch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb41d3b",
   "metadata": {},
   "source": [
    "### 2B.1 –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e3161c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rec_path = DATA_DIR / \"train_rec.csv\"\n",
    "test_rec_path = DATA_DIR / \"test_rec.csv\"\n",
    "\n",
    "if train_rec_path.exists() and test_rec_path.exists():\n",
    "    train_rec = pd.read_csv(train_rec_path)\n",
    "    test_rec = pd.read_csv(test_rec_path)\n",
    "\n",
    "    print(train_rec.head())\n",
    "    print(test_rec.head())\n",
    "else:\n",
    "    print(\"–§–∞–π–ª—ã train_rec/test_rec –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –ü—Ä–æ–ø—É—Å—Ç–∏ —ç—Ç–æ—Ç –±–ª–æ–∫ –∏–ª–∏ –¥–æ–±–∞–≤—å —Å–≤–æ–∏ —Ñ–∞–π–ª—ã.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e96cf8",
   "metadata": {},
   "source": [
    "### 2B.2 –ú–∞–ø–ø–∏–Ω–≥ ID –∏ –±–∞–∑–æ–≤—ã–π item‚Äëbias –±–µ–π–∑–ª–∞–π–Ω\n",
    "\n",
    "–ú—ã –ø—Ä–µ–¥—Å–∫–∞–∂–µ–º –æ–∂–∏–¥–∞–Ω–∏–µ —Ä–µ–π—Ç–∏–Ω–≥–∞ –∫–∞–∫:  \n",
    "`global_mean + b_user[user] + b_item[item]` (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ —É—Å–ª–æ–∂–Ω–∏—Ç—å)  \n",
    "–∏–ª–∏ –ø—Ä–æ—Å—Ç–æ —Å—Ä–µ–¥–Ω–∏–π —Ä–µ–π—Ç–∏–Ω–≥ –ø–æ item.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0bcab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'train_rec' in globals():\n",
    "    global_mean = train_rec[\"rating\"].mean()\n",
    "\n",
    "    item_mean = train_rec.groupby(\"item_id\")[\"rating\"].mean()\n",
    "    user_mean = train_rec.groupby(\"user_id\")[\"rating\"].mean()\n",
    "\n",
    "    def predict_baseline(u, i):\n",
    "        # –ü–æ–ø—ã—Ç–∫–∞ –≤–∑—è—Ç—å —Å—Ä–µ–¥–Ω–∏–π —Ä–µ–π—Ç–∏–Ω–≥ item, –ø–æ—Ç–æ–º user, –ø–æ—Ç–æ–º global\n",
    "        if i in item_mean:\n",
    "            return item_mean[i]\n",
    "        elif u in user_mean:\n",
    "            return user_mean[u]\n",
    "        else:\n",
    "            return global_mean\n",
    "\n",
    "    preds = []\n",
    "    for _, row in test_rec.iterrows():\n",
    "        u = row[\"user_id\"]\n",
    "        i = row[\"item_id\"]\n",
    "        preds.append(predict_baseline(u, i))\n",
    "\n",
    "    submission_rec = test_rec.copy()\n",
    "    submission_rec[\"rating\"] = preds\n",
    "    submission_rec_path = Path(\"./submission_rec_baseline.csv\")\n",
    "    submission_rec.to_csv(submission_rec_path, index=False)\n",
    "    submission_rec_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dea5b93",
   "metadata": {},
   "source": [
    "### 2B.3 (–û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) –ú–∞—Ç—Ä–∏—á–Ω–∞—è —Ñ–∞–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –Ω–∞ PyTorch\n",
    "\n",
    "–ï—Å–ª–∏ —É—Å–ø–µ–≤–∞–µ—à—å, –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —Ñ–∞–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—é:  \n",
    "`rating ‚âà <P_u, Q_i>`\n",
    "\n",
    "–ù–∏–∂–µ ‚Äî —Å–∫–µ–ª–µ—Ç, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–Ω–æ –¥–æ—Ä–∞–±–æ—Ç–∞—Ç—å –ø–æ–¥ –º–µ—Ç—Ä–∏–∫—É –æ–ª–∏–º–ø–∏–∞–¥—ã.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189b2fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'train_rec' in globals():\n",
    "    # –ó–∞—à–∏–≤–∞–µ–º –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é\n",
    "    unique_users = train_rec[\"user_id\"].unique()\n",
    "    unique_items = train_rec[\"item_id\"].unique()\n",
    "\n",
    "    user2idx = {u: idx for idx, u in enumerate(unique_users)}\n",
    "    item2idx = {i: idx for idx, i in enumerate(unique_items)}\n",
    "\n",
    "    train_rec[\"user_idx\"] = train_rec[\"user_id\"].map(user2idx)\n",
    "    train_rec[\"item_idx\"] = train_rec[\"item_id\"].map(item2idx)\n",
    "\n",
    "    class RecDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, df):\n",
    "            self.user_idx = df[\"user_idx\"].values\n",
    "            self.item_idx = df[\"item_idx\"].values\n",
    "            self.rating = df[\"rating\"].values.astype(\"float32\")\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.rating)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return (\n",
    "                torch.tensor(self.user_idx[idx], dtype=torch.long),\n",
    "                torch.tensor(self.item_idx[idx], dtype=torch.long),\n",
    "                torch.tensor(self.rating[idx], dtype=torch.float32),\n",
    "            )\n",
    "\n",
    "    class MFModel(nn.Module):\n",
    "        def __init__(self, n_users, n_items, n_factors=32):\n",
    "            super().__init__()\n",
    "            self.user_emb = nn.Embedding(n_users, n_factors)\n",
    "            self.item_emb = nn.Embedding(n_items, n_factors)\n",
    "            self.user_bias = nn.Embedding(n_users, 1)\n",
    "            self.item_bias = nn.Embedding(n_items, 1)\n",
    "            self.global_bias = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "        def forward(self, user_idx, item_idx):\n",
    "            u = self.user_emb(user_idx)\n",
    "            v = self.item_emb(item_idx)\n",
    "            dot = (u * v).sum(dim=1)\n",
    "            bu = self.user_bias(user_idx).squeeze(-1)\n",
    "            bi = self.item_bias(item_idx).squeeze(-1)\n",
    "            return dot + bu + bi + self.global_bias\n",
    "\n",
    "    rec_ds = RecDataset(train_rec)\n",
    "    rec_loader = torch.utils.data.DataLoader(rec_ds, batch_size=1024, shuffle=True)\n",
    "\n",
    "    n_users = len(unique_users)\n",
    "    n_items = len(unique_items)\n",
    "    mf_model = MFModel(n_users, n_items).to(DEVICE)\n",
    "    mf_optimizer = torch.optim.Adam(mf_model.parameters(), lr=1e-2)\n",
    "    mf_criterion = nn.MSELoss()\n",
    "\n",
    "    EPOCHS_MF = 5  # –º–æ–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å\n",
    "    for epoch in range(1, EPOCHS_MF + 1):\n",
    "        mf_model.train()\n",
    "        total_loss = 0\n",
    "        total_count = 0\n",
    "        for u_idx, i_idx, r in rec_loader:\n",
    "            u_idx = u_idx.to(DEVICE)\n",
    "            i_idx = i_idx.to(DEVICE)\n",
    "            r = r.to(DEVICE)\n",
    "\n",
    "            mf_optimizer.zero_grad()\n",
    "            pred = mf_model(u_idx, i_idx)\n",
    "            loss = mf_criterion(pred, r)\n",
    "            loss.backward()\n",
    "            mf_optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * r.size(0)\n",
    "            total_count += r.size(0)\n",
    "        print(f\"[MF] Epoch {epoch}: train_loss={total_loss / total_count:.4f}\")\n",
    "\n",
    "    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è —Ç–µ—Å—Ç–∞ (–µ—Å–ª–∏ item/user –Ω–µ –≤ train, fallback –Ω–∞ global_mean)\n",
    "    preds_mf = []\n",
    "    mf_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _, row in test_rec.iterrows():\n",
    "            u = row[\"user_id\"]\n",
    "            i = row[\"item_id\"]\n",
    "            if (u in user2idx) and (i in item2idx):\n",
    "                u_idx = torch.tensor([user2idx[u]], dtype=torch.long).to(DEVICE)\n",
    "                i_idx = torch.tensor([item2idx[i]], dtype=torch.long).to(DEVICE)\n",
    "                r_hat = mf_model(u_idx, i_idx).item()\n",
    "            else:\n",
    "                r_hat = global_mean\n",
    "            preds_mf.append(r_hat)\n",
    "\n",
    "    submission_rec_mf = test_rec.copy()\n",
    "    submission_rec_mf[\"rating\"] = preds_mf\n",
    "    submission_rec_mf_path = Path(\"./submission_rec_mf.csv\")\n",
    "    submission_rec_mf.to_csv(submission_rec_mf_path, index=False)\n",
    "    submission_rec_mf_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e306950a",
   "metadata": {},
   "source": [
    "---\n",
    "## 2C. –ó–∞–¥–∞—á–∞: –≥–µ–æ–¥–∞–Ω–Ω—ã–µ (latitude/longitude, —Ç–æ—á–∫–∏, —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏)\n",
    "\n",
    "–ß–∞—Å—Ç—ã–π —Ñ–æ—Ä–º–∞—Ç: –µ—Å—Ç—å –æ–±—ä–µ–∫—Ç—ã —Å –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏, –Ω–∞–¥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å —Ü–µ–Ω—É/–∫–ª–∞—Å—Å/–≤—Ä–µ–º—è –∏ —Ç.–ø.\n",
    "\n",
    "–ù–∞–ø—Ä–∏–º–µ—Ä, `train_geo.csv` –∏ `test_geo.csv`:\n",
    "\n",
    "- `lat`, `lon` ‚Äî –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã —Ç–æ—á–∫–∏ (–∏–ª–∏ –Ω–∞—á–∞–ª–æ/–∫–æ–Ω–µ—Ü: `lat1`, `lon1`, `lat2`, `lon2`)\n",
    "- –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
    "- `target` (—Ç–æ–ª—å–∫–æ –≤ train)\n",
    "\n",
    "–ë–µ–π–∑–ª–∞–π–Ω: —Å–æ–∑–¥–∞–¥–∏–º —Ñ–∏—á–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –ø–æ —Ñ–æ—Ä–º—É–ª–µ –≥–∞–≤–µ—Ä—Å–∏–Ω–∞) –∏ –æ–±—É—á–∏–º **RandomForest** –∏–∑ `sklearn`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca93ede8",
   "metadata": {},
   "source": [
    "### 2C.1 –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba5b45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_geo_path = DATA_DIR / \"train_geo.csv\"\n",
    "test_geo_path = DATA_DIR / \"test_geo.csv\"\n",
    "\n",
    "if train_geo_path.exists() and test_geo_path.exists():\n",
    "    train_geo = pd.read_csv(train_geo_path)\n",
    "    test_geo = pd.read_csv(test_geo_path)\n",
    "\n",
    "    print(train_geo.head())\n",
    "    print(test_geo.head())\n",
    "else:\n",
    "    print(\"–§–∞–π–ª—ã train_geo/test_geo –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –ü—Ä–æ–ø—É—Å—Ç–∏ —ç—Ç–æ—Ç –±–ª–æ–∫ –∏–ª–∏ –¥–æ–±–∞–≤—å —Å–≤–æ–∏ —Ñ–∞–π–ª—ã.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c54198",
   "metadata": {},
   "source": [
    "### 2C.2 –•–µ–ª–ø–µ—Ä: —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –ø–æ —Ñ–æ—Ä–º—É–ª–µ –≥–∞–≤–µ—Ä—Å–∏–Ω–∞\n",
    "\n",
    "–ï—Å–ª–∏ –µ—Å—Ç—å –Ω–∞—á–∞–ª–æ –∏ –∫–æ–Ω–µ—Ü (`lat1, lon1, lat2, lon2`), —Å–æ–∑–¥–∞–¥–∏–º —Ñ–∏—á—É `distance_km`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f55ad4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    # –í—Ö–æ–¥: –≥—Ä–∞–¥—É—Å—ã\n",
    "    R = 6371.0  # km\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "\n",
    "    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    return R * c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68eaf27",
   "metadata": {},
   "source": [
    "### 2C.3 –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ñ–∏—á –∏ –æ–±—É—á–µ–Ω–∏–µ RandomForest\n",
    "\n",
    "–ó–¥–µ—Å—å –ø—Ä–∏–º–µ—Ä –¥–ª—è —Å–ª—É—á–∞—è —Å –Ω–∞—á–∞–ª–æ–º –∏ –∫–æ–Ω—Ü–æ–º (`lat1, lon1, lat2, lon2`).  \n",
    "–ï—Å–ª–∏ —É —Ç–µ–±—è –¥—Ä—É–≥–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞, –ø–µ—Ä–µ—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–∏ —Ñ–∏—á–∏.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadc0535",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "\n",
    "if 'train_geo' in globals():\n",
    "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ lat1/lon1/lat2/lon2\n",
    "    has_traj = all(col in train_geo.columns for col in [\"lat1\", \"lon1\", \"lat2\", \"lon2\"])\n",
    "\n",
    "    if has_traj:\n",
    "        train_geo[\"distance_km\"] = haversine_distance(\n",
    "            train_geo[\"lat1\"], train_geo[\"lon1\"], train_geo[\"lat2\"], train_geo[\"lon2\"]\n",
    "        )\n",
    "        test_geo[\"distance_km\"] = haversine_distance(\n",
    "            test_geo[\"lat1\"], test_geo[\"lon1\"], test_geo[\"lat2\"], test_geo[\"lon2\"]\n",
    "        )\n",
    "\n",
    "    # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ target ‚Äî —á–∏—Å–ª–æ–≤–æ–π (—Ä–µ–≥—Ä–µ—Å—Å–∏—è).\n",
    "    # –ï—Å–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è, –º–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ RandomForestClassifier –∏ target –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ int.\n",
    "    target_col = \"target\"\n",
    "    feature_cols = [c for c in train_geo.columns if c not in [target_col]]\n",
    "\n",
    "    X_train = train_geo[feature_cols]\n",
    "    y_train = train_geo[target_col]\n",
    "    X_test = test_geo[feature_cols]\n",
    "\n",
    "    is_classification = (y_train.dtype == \"object\") or (len(y_train.unique()) < 20)\n",
    "\n",
    "    if is_classification:\n",
    "        # –ø–µ—Ä–µ–∫–æ–¥–∏—Ä—É–µ–º —Ü–µ–ª–µ–≤–æ–π –∫–ª–∞—Å—Å\n",
    "        classes = sorted(y_train.unique())\n",
    "        class2id = {cls: i for i, cls in enumerate(classes)}\n",
    "        id2class = {i: cls for cls, i in class2id.items()}\n",
    "\n",
    "        y_train_enc = y_train.map(class2id)\n",
    "\n",
    "        clf = RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=15,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        clf.fit(X_train, y_train_enc)\n",
    "        y_test_pred_id = clf.predict(X_test)\n",
    "        y_test_pred = [id2class[i] for i in y_test_pred_id]\n",
    "    else:\n",
    "        reg = RandomForestRegressor(\n",
    "            n_estimators=300,\n",
    "            max_depth=15,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        reg.fit(X_train, y_train)\n",
    "        y_test_pred = reg.predict(X_test)\n",
    "\n",
    "    submission_geo = test_geo.copy()\n",
    "    submission_geo[target_col] = y_test_pred\n",
    "    submission_geo_path = Path(\"./submission_geo.csv\")\n",
    "    submission_geo.to_csv(submission_geo_path, index=False)\n",
    "    submission_geo_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c19d60",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. –ö–∞–∫ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å —ç—Ç–æ—Ç –±–ª–æ–∫–Ω–æ—Ç –ø–æ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –∑–∞–¥–∞—á—É –æ–ª–∏–º–ø–∏–∞–¥—ã\n",
    "\n",
    "1. **–°—Ä–∞–∑—É –ø–æ—Å–º–æ—Ç—Ä–∏ —Ñ–æ—Ä–º–∞—Ç—ã —Ñ–∞–π–ª–æ–≤**, –∫–æ—Ç–æ—Ä—ã–µ –¥–∞—é—Ç –æ—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä—ã.\n",
    "2. –í —ç—Ç–æ–º –±–ª–æ–∫–Ω–æ—Ç–µ –ø–æ–¥—Å—Ç–∞–≤—å:\n",
    "   - –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º (`DATA_DIR`, –∏–º–µ–Ω–∞ CSV);\n",
    "   - –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫ (`prompt`, `response`, `path`, `label`, `user_id`, `item_id`, `lat1`, `lon1`, –∏ —Ç.–¥.);\n",
    "   - —Ü–µ–ª–µ–≤—É—é –º–µ—Ç—Ä–∏–∫—É (–¥–ª—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π/–≥–µ–æ–¥–∞–Ω–Ω—ã—Ö/–∞—É–¥–∏–æ).\n",
    "3. –ü—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ **—É–ø—Ä–æ—Å—Ç–∏—Ç—å**:\n",
    "   - —É–±—Ä–∞—Ç—å –ª–∏—à–Ω–∏–µ —Å–µ–∫—Ü–∏–∏ (–µ—Å–ª–∏ –∑–∞–¥–∞—á–∞ —Ç–æ–ª—å–∫–æ LLM –∏–ª–∏ —Ç–æ–ª—å–∫–æ –∑–≤—É–∫);\n",
    "   - —Å–Ω–∏–∑–∏—Ç—å —Ä–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö.\n",
    "4. –ü—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ **—É—Å–∏–ª–∏—Ç—å**:\n",
    "   - –¥–æ–±–∞–≤–∏—Ç—å –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–ª—è –∞—É–¥–∏–æ;\n",
    "   - –¥–æ–±–∞–≤–∏—Ç—å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é, –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é, —Ñ–∏—á–∏ –¥–ª—è –≥–µ–æ–¥–∞–Ω–Ω—ã—Ö;\n",
    "   - –∑–∞–º–µ–Ω–∏—Ç—å –±–µ–π–∑–ª–∞–π–Ω–æ–≤—É—é LLM –Ω–∞ –±–æ–ª–µ–µ –º–æ—â–Ω—É—é/—Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –ø–æ —É—Å–ª–æ–≤–∏—é.\n",
    "\n",
    "–£–¥–∞—á–∏ –Ω–∞ –æ–ª–∏–º–ø–∏–∞–¥–µ! üí™"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
